{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.345362\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x, W) = - \\log{(truescore(\\frac{X.dot(W)}{sum(X.dot(W))}))} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3073), (3073, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4055826558478217\n",
      "[[ -4.19670602  28.40761452  -3.45568219 ... -18.12122782 -24.14348344\n",
      "  -53.10404278]\n",
      " [ -4.87149289  31.49540657  -7.80340968 ... -21.23371776 -25.29159951\n",
      "  -53.47359996]\n",
      " [ -4.00125805  34.69208264 -10.13795198 ... -24.48288757 -26.8513872\n",
      "  -56.8727704 ]\n",
      " ...\n",
      " [ -0.83919133   4.08942176  -9.78945364 ...   5.23860806 -22.46878681\n",
      "  -44.71419916]\n",
      " [  2.67989202   7.47984659 -10.22568158 ...   2.98665111 -23.98608068\n",
      "  -49.3042809 ]\n",
      " [ -1.40397396  -1.46970459  -1.36763732 ...  -1.39557877  -1.44232003\n",
      "   -1.71540541]]\n"
     ]
    }
   ],
   "source": [
    "reg = 5e1\n",
    "dW= np.zeros(W.shape)\n",
    "scores = np.zeros((X_dev.shape[0], W.shape[1]))\n",
    "for i in range(0, X_dev.shape[0]):\n",
    "    for j in range(0, W.shape[1]):\n",
    "        dot = np.sum(X_dev[i, :]*(W[:, j])) #1 (1)\n",
    "        scores[i, j] = dot  #(1)\n",
    "    scores[i, :] -= np.max(scores[i, :]) #avoiding numerical stabilization\n",
    "    scores_exp = np.exp(scores[i, :]) #2 (1,10)\n",
    "    num = scores_exp #3 #numerator (1,10)\n",
    "    scores_sum = scores_exp.sum() #3 (1)\n",
    "    den = scores_sum #3 (1)\n",
    "    invden = 1 / den #3.5 #denumerator > (1) \n",
    "    probs = num * invden #4 #sigmoid is done (1, 10) #broadcast\n",
    "    tclass_prob = probs[y_dev[i]] #5 (1)\n",
    "    tclass_lprob = np.log(tclass_prob) #6 (1)\n",
    "    loss += -(tclass_lprob) #7 (1)\n",
    "    \n",
    "    #derivative with chain rule\n",
    "    dloss = 1\n",
    "    dtclass_lprob = -1*dloss #7 # shape > 1\n",
    "    dtclass_prob = 1/tclass_prob*dtclass_lprob #6  # shape > 1\n",
    "    \n",
    "    dnum = invden*dtclass_prob #(1)\n",
    "    dinvden = num*dtclass_prob #(1,10) #broadcast\n",
    "    \n",
    "    dden = (-1 / den**2)*dinvden #(1,10) #broadcast\n",
    "    dscores_sum = 1*dden #(1,10)\n",
    "    dscores_exp = 1*dnum #(1)\n",
    "    dscores_exp += dscores_exp*dscores_sum #(1,10)broadcast\n",
    "    \n",
    "    dscores = np.exp(scores[i, :])*dscores_exp #(1,10)\n",
    "    ddot = dscores #(1,10)\n",
    "    dW += X_dev[i, :].reshape(-1,1).dot(dscores.reshape(1,-1)) # dscores > (1,10) dW > 3073x10\n",
    "    \n",
    "#     dscores_exp = inv_scores_sum*dnormalized_scores #4 # 1 number\n",
    "#     dinv_scores_sum = scores_exp*dnormalized_scores #4 # (1,10), true class score affect all of the classes?\n",
    "    \n",
    "#     dscores_sum = (-1/scores_sum**2)*dinv_scores_sum #4\n",
    "#     dscores_exp += dscores_sum.sum() #3\n",
    "#     dscores_i = np.exp(scores[i, :])*dscores_exp #2\n",
    "#     ddot = scores[i, :] * dscores_i #1\n",
    "#     dW += X_dev[i, :].reshape(W.shape[0],-1) * ddot.reshape(1, -1) \n",
    "#     #ddot*X_dev[i, :]\n",
    "    \n",
    "#     #ddot = ((1 - scores[i, y_dev[i]])*scores[i, y_dev[i]])*d_loss\n",
    "#     #dW[:, y_dev[i]] += X_dev[i, :] * ddot\n",
    "loss = loss / X_dev.shape[0]\n",
    "loss += reg*np.sum(W*W)\n",
    "dW = dW / X_dev.shape[0]\n",
    "dW += reg*2*W\n",
    "print(loss)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3073,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev[1, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Initially all weights are close to 0, so scores. If we exponentiate the scores, we get 1 for each class and example. Then we normalize them, they are going to be 1/number of classes. And loss is -log(1/number of classes) which in this case is -log(0.1)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.189089 analytic: 12.588178, relative error: 1.000000e+00\n",
      "numerical: -0.444305 analytic: -8.202051, relative error: 8.972273e-01\n",
      "numerical: -4.457800 analytic: 17.383809, relative error: 1.000000e+00\n",
      "numerical: -5.255162 analytic: 8.396065, relative error: 1.000000e+00\n",
      "numerical: -0.342034 analytic: 4.335069, relative error: 1.000000e+00\n",
      "numerical: 5.286925 analytic: -12.207131, relative error: 1.000000e+00\n",
      "numerical: -1.677300 analytic: -16.216522, relative error: 8.125274e-01\n",
      "numerical: -1.545366 analytic: 15.928681, relative error: 1.000000e+00\n",
      "numerical: 2.719497 analytic: -16.419000, relative error: 1.000000e+00\n",
      "numerical: 2.145486 analytic: -11.246855, relative error: 1.000000e+00\n",
      "numerical: 1.592355 analytic: -2.617703, relative error: 1.000000e+00\n",
      "numerical: -0.204121 analytic: -4.956848, relative error: 9.208981e-01\n",
      "numerical: 1.639391 analytic: 3.441682, relative error: 3.547068e-01\n",
      "numerical: -0.904669 analytic: 11.892206, relative error: 1.000000e+00\n",
      "numerical: -0.757003 analytic: -0.225335, relative error: 5.412277e-01\n",
      "numerical: 0.781189 analytic: 2.341920, relative error: 4.997363e-01\n",
      "numerical: 2.137765 analytic: -23.030102, relative error: 1.000000e+00\n",
      "numerical: 0.845727 analytic: 9.883975, relative error: 8.423578e-01\n",
      "numerical: 0.049289 analytic: -4.832126, relative error: 1.000000e+00\n",
      "numerical: -1.146000 analytic: 13.576146, relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.312187e+00 computed in 0.124292s\n",
      "vectorized loss: 2.312187e+00 computed in 0.001800s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3121870572013106 without regularization\n"
     ]
    }
   ],
   "source": [
    "#vectorized loss\n",
    "N = X_dev.shape[0] #number of examples\n",
    "scores_dev = X_dev.dot(W) #1\n",
    "scores_dev -= np.max(scores_dev, axis = 1).reshape(N, 1) #for numerical stability\n",
    "exp_dev = np.exp(scores_dev) #2\n",
    "sum_scores_dev = exp_dev.sum(axis=1).reshape(N, 1) #3\n",
    "inv_sum_scores_dev = 1 /sum_scores_dev #4\n",
    "probs = exp_dev *inv_sum_scores_dev #5\n",
    "true_class_probs = probs[range(0,N), y_dev] #6\n",
    "losses = -np.log(true_class_probs) #7\n",
    "total_loss = np.sum(losses) / N # it is average over all examples\n",
    "print(total_loss, \"without regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(true_class_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3073), (3073, 10), (500, 10))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.shape, W.shape, scores_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (500,10) (500,500) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bf5659603c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md_sum_scores_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum_scores_dev\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdinv_sum_scores_dev\u001b[0m \u001b[0;31m#4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdexp_dev_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md_sum_scores_dev\u001b[0m \u001b[0;31m#3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdscores_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdexp_dev\u001b[0m \u001b[0;31m#2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdscores_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (500,10) (500,500) "
     ]
    }
   ],
   "source": [
    "dlosses = - 1 / true_class_probs #7\n",
    "d_true_class_probs = 1*dlosses #6 \n",
    "dexp_dev = inv_sum_scores_dev*d_true_class_probs #5\n",
    "dinv_sum_scores_dev = exp_dev.sum(axis=1)*d_true_class_probs #5\n",
    "d_sum_scores_dev = (-1 / sum_scores_dev**2)*dinv_sum_scores_dev #4\n",
    "dexp_dev_sum = 1*d_sum_scores_dev #3\n",
    "dscores_dev = np.exp(scores_dev)*dexp_dev #2\n",
    "dW = X_dev.T.dot(dscores_dev)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.24344591, -10.0693846 ,  -7.21488314, -10.05059846,\n",
       "        -9.6885657 ,  -9.09567875, -13.19385224, -10.75194813,\n",
       "        -7.59678406,  -7.09782924, -10.38920684,  -8.44498651,\n",
       "       -11.71486451, -17.86464526, -12.39219935,  -4.26318896,\n",
       "        -5.62145284, -10.51795405,  -7.42975185, -18.20841223,\n",
       "       -12.16054727, -10.52370702, -13.54912635, -15.2144803 ,\n",
       "        -9.81524152, -11.78209259, -15.52690029, -15.75998632,\n",
       "        -9.35495608, -10.44093938, -11.25489778, -14.70446588,\n",
       "       -12.59559697, -11.11331942,  -9.31502842, -25.0585745 ,\n",
       "       -12.95305913,  -8.25955757,  -4.94753002,  -6.81677992,\n",
       "       -13.36469739,  -8.49564372, -11.4601513 ,  -7.23243183,\n",
       "        -7.72567799,  -8.94667179, -18.91175583, -12.13044704,\n",
       "        -7.93887352, -15.43127958, -12.04393639, -13.64336076,\n",
       "       -12.42898409,  -8.77216919, -13.15067272, -13.14381448,\n",
       "        -8.40257197,  -7.11853199,  -7.07776234, -16.09172783,\n",
       "        -9.91562211, -12.2269092 ,  -5.05994696, -10.10531559,\n",
       "        -8.02760939, -18.15220735, -11.00948501,  -4.87015315,\n",
       "       -11.65693975,  -5.47931899, -12.91774886, -11.64609286,\n",
       "       -11.94634848,  -9.43926632, -11.53931317,  -8.18702078,\n",
       "        -8.4081009 , -13.00286869,  -9.99883388,  -9.03256084,\n",
       "       -12.93951054, -14.03234266, -12.78466442, -14.3668561 ,\n",
       "       -13.40066   , -14.33297711,  -6.07029646, -11.30609065,\n",
       "       -16.39698941,  -8.53979211, -10.53920527, -11.65106209,\n",
       "        -7.50443925,  -9.85136952, -11.08536843, -13.47613216,\n",
       "        -8.50384475, -12.03026128,  -8.68069942, -11.10655972,\n",
       "       -12.03801771, -13.63167347,  -5.57367983, -11.4823469 ,\n",
       "        -5.27982227, -54.90187052,  -7.36062223, -10.00294153,\n",
       "       -15.92209205,  -7.17261805, -11.63790164,  -7.15494869,\n",
       "       -13.86060187, -12.65457047, -12.05067617,  -4.34701836,\n",
       "        -8.90717305, -12.95897749, -16.91640317, -14.56836173,\n",
       "        -9.17745086,  -9.67130751,  -8.30242446,  -9.48322854,\n",
       "       -13.20501497,  -9.08838435,  -8.62251367, -11.80130798,\n",
       "        -7.98015578, -10.68461194,  -9.038466  ,  -8.43647165,\n",
       "        -8.30413128,  -8.495076  , -21.45053063, -10.30163198,\n",
       "       -14.87066569,  -4.49920604, -13.50936859, -10.79377223,\n",
       "       -14.57612526, -11.6656774 ,  -6.32068519, -11.70956676,\n",
       "       -13.67724327,  -8.88848119,  -7.50960001,  -8.78769436,\n",
       "        -9.01278409, -38.88319151,  -9.40827838, -12.56972081,\n",
       "       -10.49886209,  -5.92731147,  -7.39049467, -17.44904821,\n",
       "       -14.45892248, -20.81541007,  -9.01153144, -16.37703273,\n",
       "       -10.74416464,  -9.87781171, -11.73980525, -10.05007951,\n",
       "        -7.64034337, -11.61333616,  -7.82848437, -11.01007304,\n",
       "        -5.39960105, -15.39602403,  -8.71659592,  -6.74519875,\n",
       "        -6.51091814,  -7.39414113,  -8.81780991,  -7.10115626,\n",
       "        -7.86866768,  -8.62979074, -10.16700854, -11.07179471,\n",
       "        -9.17292705,  -7.9951894 ,  -9.59574101, -10.39273351,\n",
       "        -5.2236573 ,  -5.74906626, -11.84702783,  -6.84170363,\n",
       "        -7.45390556, -14.04650333,  -5.45980791,  -7.99598842,\n",
       "        -9.65578722, -10.82661184,  -5.569454  ,  -8.06769491,\n",
       "        -7.00414421,  -7.45600049, -12.38496381, -10.87083948,\n",
       "       -11.81576182, -13.51959639, -15.19941104,  -8.29853468,\n",
       "       -12.3819311 , -10.48980121, -10.3796521 , -11.41030226,\n",
       "       -12.43618876,  -9.74442978,  -5.26232354, -11.66041006,\n",
       "       -30.65694543, -15.26626512,  -6.12384834,  -8.16098019,\n",
       "       -17.58731148, -12.59859004, -16.62383505,  -6.43680272,\n",
       "       -10.56925276, -18.67625605, -11.21000193, -13.48815833,\n",
       "        -6.29841498,  -7.70340129,  -8.66299943, -10.21892679,\n",
       "        -9.97823457, -13.93177835,  -8.3863978 ,  -4.91326229,\n",
       "        -4.83778975, -11.93493743, -12.40294854,  -8.20920923,\n",
       "       -10.87154942, -16.45842608, -17.13815562,  -5.09097937,\n",
       "       -11.36589716,  -7.44670543,  -4.46937702,  -4.19243597,\n",
       "       -14.89077676, -10.38492443,  -9.57577523,  -6.46967595,\n",
       "       -14.2259185 , -14.58577084, -10.67381647, -35.25985814,\n",
       "        -9.35458041,  -9.34049588,  -5.12084335,  -8.3386323 ,\n",
       "       -18.37079616,  -8.82111263, -11.13422453, -10.88811685,\n",
       "       -11.46609861,  -7.85373826,  -8.57116528, -13.86809526,\n",
       "        -9.7396135 , -10.85367424,  -6.43712366,  -8.11640457,\n",
       "        -8.27634887,  -6.98755275, -12.63995054, -12.05279431,\n",
       "       -11.90537374,  -8.34445833,  -9.16335799,  -9.17453726,\n",
       "       -13.33734557,  -3.69949148,  -6.67925645,  -8.68319354,\n",
       "       -13.06805416,  -9.4389541 ,  -9.07061964,  -7.10435987,\n",
       "       -12.14977041,  -7.74731749,  -8.91798285, -10.58662983,\n",
       "       -12.31534058, -10.97987042, -17.51334657,  -9.09648734,\n",
       "       -16.37573691, -17.94292517,  -8.9513626 , -11.952347  ,\n",
       "       -10.64587274,  -8.41433158, -13.14860836,  -6.10857013,\n",
       "       -12.16870401,  -6.95177332,  -9.5851814 ,  -7.07176828,\n",
       "       -19.99642247, -12.67987689,  -4.55057038, -10.78476396,\n",
       "       -13.12457134, -13.546132  , -12.90386522, -10.1084117 ,\n",
       "       -15.04152555, -10.56890574,  -6.68945335,  -6.91555042,\n",
       "       -15.3412138 ,  -8.91575875,  -6.98129141, -10.67591502,\n",
       "        -5.84656973, -10.70498232,  -9.7665601 , -10.29622613,\n",
       "       -10.39759423, -12.47183334, -11.4929108 , -14.94510038,\n",
       "        -6.39821265, -10.20438531, -13.20673335, -14.18450232,\n",
       "       -11.82536363,  -7.27736421,  -6.25292773, -12.77198418,\n",
       "       -12.46381725, -19.33795731, -14.35347691,  -9.85374983,\n",
       "       -10.29621035,  -9.15800969, -15.22465611,  -8.77661388,\n",
       "       -16.51640839,  -5.65575687, -12.31509025, -12.27770233,\n",
       "        -7.99885948, -11.55256432,  -9.13529955, -11.17900578,\n",
       "        -9.14714933,  -9.42350163,  -8.96678693,  -8.27053376,\n",
       "        -9.60366156, -11.10043584, -12.08147832, -13.36643496,\n",
       "        -3.08357526, -15.68683532, -10.63582598,  -4.54826909,\n",
       "       -27.31961029, -11.29205286, -13.27492397, -10.86373626,\n",
       "        -9.62473989,  -4.40724208, -24.6768355 ,  -9.03808895,\n",
       "       -11.95429315,  -9.85660303, -22.75853187,  -8.35144754,\n",
       "       -10.55629274,  -8.99042738,  -8.99442944,  -6.55253997,\n",
       "       -11.48186373,  -7.37553144, -12.05210228,  -7.91154194,\n",
       "        -5.63383527, -14.20323211, -11.2920136 , -10.21299183,\n",
       "       -12.22887926,  -9.55664168,  -8.15190183, -12.60711026,\n",
       "        -9.01687961, -13.61277462,  -8.66740839, -12.74041773,\n",
       "        -3.40870868,  -5.93176865, -11.06352997, -15.70271054,\n",
       "        -9.10019064, -10.55531636, -10.42759354,  -9.9744737 ,\n",
       "        -6.35252498, -11.14735294, -24.60321758, -17.26097775,\n",
       "       -13.02518824,  -2.88871261,  -8.10194848,  -9.23835394,\n",
       "        -9.64678933,  -9.1376025 ,  -8.09922469,  -9.18521764,\n",
       "        -8.64322384, -11.95263509,  -6.13890991, -12.53587706,\n",
       "        -5.59147311, -11.34601468,  -9.53600832,  -5.77286492,\n",
       "        -9.35306848, -11.98885933,  -8.69773876,  -6.54967484,\n",
       "        -4.90497495,  -9.86672402, -10.30383974,  -8.50686725,\n",
       "       -10.73687993,  -8.29709161, -10.61722884, -10.58216223,\n",
       "       -12.07181851, -12.04517463,  -8.47648985, -11.15127461,\n",
       "        -5.26539821,  -9.01356284, -14.42911436, -10.06675249,\n",
       "        -8.67011468,  -7.8786855 ,  -7.03543773, -10.5929471 ,\n",
       "        -7.68442294, -12.22748779, -10.70637981,  -9.84430576,\n",
       "       -11.85372717, -10.81144125,  -9.51986918, -10.2641849 ,\n",
       "        -7.53954927, -10.86616227, -14.56859945, -13.23421742,\n",
       "        -5.53587825,  -5.64104267, -17.89203805,  -8.6752569 ,\n",
       "       -10.72610589,  -8.39301612, -17.45500618, -11.78472113,\n",
       "        -4.23386512,  -7.95876537, -12.30131596, -11.03889766,\n",
       "       -11.75998848, -13.65279875, -21.31781043, -23.93004747,\n",
       "       -12.29947513,  -4.34580201,  -6.43887347, -11.07295239,\n",
       "        -6.72988713,  -4.37470145,  -8.00124125,  -5.52292634,\n",
       "        -8.70580258,  -9.58669289, -10.85852134,  -9.57509663,\n",
       "       -13.20812855, -28.81826884, -10.9572795 ,  -7.08386625,\n",
       "       -10.52473313,  -8.63735604,  -8.5204342 , -10.50217685,\n",
       "        -9.9863721 ,  -8.90915189,  -7.84971733,  -8.6909036 ])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloss_dev = -1/ probs[range(0,X_dev.shape[0]), y_dev]\n",
    "dloss_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# Your code\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
